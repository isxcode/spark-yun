package com.isxcode.spark.modules.work.run.impl;

import com.alibaba.fastjson.JSON;
import com.isxcode.spark.api.agent.constants.AgentType;
import com.isxcode.spark.api.agent.constants.SparkAgentUrl;
import com.isxcode.spark.api.agent.req.spark.*;
import com.isxcode.spark.api.agent.res.spark.GetWorkStderrLogRes;
import com.isxcode.spark.api.api.constants.PathConstants;
import com.isxcode.spark.api.cluster.constants.ClusterNodeStatus;
import com.isxcode.spark.api.cluster.dto.ScpFileEngineNodeDto;
import com.isxcode.spark.api.datasource.dto.ConnectInfo;
import com.isxcode.spark.api.instance.constants.InstanceStatus;
import com.isxcode.spark.api.work.constants.WorkLog;
import com.isxcode.spark.api.work.constants.WorkType;
import com.isxcode.spark.backend.api.base.exceptions.WorkRunException;
import com.isxcode.spark.api.work.dto.ClusterConfig;
import com.isxcode.spark.api.work.res.RunWorkRes;
import com.isxcode.spark.backend.api.base.exceptions.IsxAppException;
import com.isxcode.spark.backend.api.base.pojos.BaseResponse;
import com.isxcode.spark.backend.api.base.properties.IsxAppProperties;
import com.isxcode.spark.common.locker.Locker;
import com.isxcode.spark.common.utils.aes.AesUtils;
import com.isxcode.spark.common.utils.http.HttpUrlUtils;
import com.isxcode.spark.common.utils.http.HttpUtils;
import com.isxcode.spark.common.utils.path.PathUtils;
import com.isxcode.spark.modules.alarm.service.AlarmService;
import com.isxcode.spark.modules.cluster.entity.ClusterEntity;
import com.isxcode.spark.modules.cluster.entity.ClusterNodeEntity;
import com.isxcode.spark.modules.cluster.mapper.ClusterNodeMapper;
import com.isxcode.spark.modules.cluster.repository.ClusterNodeRepository;
import com.isxcode.spark.modules.cluster.repository.ClusterRepository;
import com.isxcode.spark.modules.datasource.entity.DatasourceEntity;
import com.isxcode.spark.modules.datasource.mapper.DatasourceMapper;
import com.isxcode.spark.modules.datasource.repository.DatasourceRepository;
import com.isxcode.spark.modules.datasource.service.DatasourceService;
import com.isxcode.spark.modules.datasource.source.DataSourceFactory;
import com.isxcode.spark.modules.datasource.source.Datasource;
import com.isxcode.spark.modules.file.entity.FileEntity;
import com.isxcode.spark.modules.file.repository.FileRepository;
import com.isxcode.spark.modules.func.entity.FuncEntity;
import com.isxcode.spark.modules.func.mapper.FuncMapper;
import com.isxcode.spark.modules.func.repository.FuncRepository;
import com.isxcode.spark.modules.secret.entity.SecretKeyEntity;
import com.isxcode.spark.modules.secret.repository.SecretKeyRepository;
import com.isxcode.spark.modules.work.entity.WorkConfigEntity;
import com.isxcode.spark.modules.work.entity.WorkEntity;
import com.isxcode.spark.modules.work.entity.WorkEventEntity;
import com.isxcode.spark.modules.work.entity.WorkInstanceEntity;
import com.isxcode.spark.modules.work.repository.*;
import com.isxcode.spark.modules.work.run.WorkExecutor;
import com.isxcode.spark.modules.work.run.WorkRunContext;
import com.isxcode.spark.modules.work.run.WorkRunJobFactory;
import com.isxcode.spark.modules.work.sql.SqlCommentService;
import com.isxcode.spark.modules.work.sql.SqlFunctionService;
import com.isxcode.spark.modules.work.sql.SqlValueService;
import com.isxcode.spark.modules.workflow.repository.WorkflowInstanceRepository;
import com.jcraft.jsch.JSchException;
import com.jcraft.jsch.SftpException;
import lombok.extern.slf4j.Slf4j;
import org.apache.commons.lang3.StringUtils;
import org.apache.logging.log4j.util.Strings;
import org.quartz.Scheduler;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Service;
import org.springframework.web.client.HttpServerErrorException;
import org.springframework.web.client.ResourceAccessException;

import java.io.File;
import java.io.IOException;
import java.time.LocalDateTime;
import java.util.*;

import static com.isxcode.spark.common.utils.ssh.SshUtils.scpJar;

@Service
@Slf4j
public class SparkSqlExecutor extends WorkExecutor {

    private final WorkInstanceRepository workInstanceRepository;

    private final ClusterRepository clusterRepository;

    private final ClusterNodeRepository clusterNodeRepository;

    private final WorkRepository workRepository;

    private final WorkConfigRepository workConfigRepository;

    private final Locker locker;

    private final HttpUrlUtils httpUrlUtils;

    private final FuncRepository funcRepository;

    private final FuncMapper funcMapper;

    private final ClusterNodeMapper clusterNodeMapper;

    private final AesUtils aesUtils;

    private final IsxAppProperties isxAppProperties;

    private final FileRepository fileRepository;

    private final DatasourceService datasourceService;

    private final SqlCommentService sqlCommentService;

    private final SqlValueService sqlValueService;

    private final SqlFunctionService sqlFunctionService;

    private final DatasourceMapper datasourceMapper;

    private final DataSourceFactory dataSourceFactory;

    private final SecretKeyRepository secretKeyRepository;

    public SparkSqlExecutor(WorkInstanceRepository workInstanceRepository,
        WorkflowInstanceRepository workflowInstanceRepository, DatasourceRepository datasourceRepository,
        SqlCommentService sqlCommentService, SqlValueService sqlValueService, SqlFunctionService sqlFunctionService,
        AlarmService alarmService, DataSourceFactory dataSourceFactory, DatasourceMapper datasourceMapper,
        SecretKeyRepository secretKeyRepository, WorkEventRepository workEventRepository, Scheduler scheduler,
        Locker locker, WorkRepository workRepository, WorkRunJobFactory workRunJobFactory,
        WorkConfigRepository workConfigRepository, VipWorkVersionRepository vipWorkVersionRepository,
        ClusterNodeMapper clusterNodeMapper, AesUtils aesUtils, ClusterNodeRepository clusterNodeRepository,
        ClusterRepository clusterRepository, HttpUrlUtils httpUrlUtils, FuncRepository funcRepository,
        FuncMapper funcMapper, IsxAppProperties isxAppProperties, FileRepository fileRepository,
        DatasourceService datasourceService) {

        super(alarmService, scheduler, locker, workRepository, workInstanceRepository, workflowInstanceRepository,
            workEventRepository, workRunJobFactory, sqlFunctionService, workConfigRepository, vipWorkVersionRepository);
        this.workInstanceRepository = workInstanceRepository;
        this.clusterRepository = clusterRepository;
        this.clusterNodeRepository = clusterNodeRepository;
        this.workRepository = workRepository;
        this.workConfigRepository = workConfigRepository;
        this.locker = locker;
        this.httpUrlUtils = httpUrlUtils;
        this.funcRepository = funcRepository;
        this.funcMapper = funcMapper;
        this.clusterNodeMapper = clusterNodeMapper;
        this.aesUtils = aesUtils;
        this.isxAppProperties = isxAppProperties;
        this.fileRepository = fileRepository;
        this.datasourceService = datasourceService;
        this.sqlCommentService = sqlCommentService;
        this.sqlValueService = sqlValueService;
        this.sqlFunctionService = sqlFunctionService;
        this.datasourceMapper = datasourceMapper;
        this.dataSourceFactory = dataSourceFactory;
        this.secretKeyRepository = secretKeyRepository;
    }

    @Override
    public String getWorkType() {
        return WorkType.QUERY_SPARK_SQL;
    }

    @Override
    protected String execute(WorkRunContext workRunContext, WorkInstanceEntity workInstance,
        WorkEventEntity workEvent) {

        // è·å–å®ä¾‹æ—¥å¿—
        StringBuilder logBuilder = new StringBuilder(workInstance.getSubmitLog());

        // æ‰“å°é¦–è¡Œæ—¥å¿—
        if (workEvent.getEventProcess() == 0) {
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹ç”³è¯·é›†ç¾¤èµ„æº"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // æ£€æŸ¥é›†ç¾¤
        if (workEvent.getEventProcess() == 1) {

            // æ£€æµ‹é›†ç¾¤æ˜¯å¦é…ç½®
            if (Strings.isEmpty(workRunContext.getClusterConfig().getClusterId())) {
                throw new WorkRunException(errorLog("âš ï¸ ç”³è¯·èµ„æºå¤±è´¥ : è®¡ç®—å¼•æ“æœªé…ç½®"));
            }

            // æ£€æŸ¥é›†ç¾¤æ˜¯å¦å­˜åœ¨
            ClusterEntity cluster = clusterRepository.findById(workRunContext.getClusterConfig().getClusterId())
                .orElseThrow(() -> new WorkRunException(errorLog("âš ï¸ ç”³è¯·èµ„æºå¤±è´¥ : è®¡ç®—å¼•æ“ä¸å­˜åœ¨")));

            // æ£€æµ‹é›†ç¾¤ä¸­æ˜¯å¦ä¸ºç©º
            List<ClusterNodeEntity> clusterNodes =
                clusterNodeRepository.findAllByClusterIdAndStatus(cluster.getId(), ClusterNodeStatus.RUNNING);
            if (clusterNodes.isEmpty()) {
                throw new WorkRunException(errorLog("âš ï¸ ç”³è¯·èµ„æºå¤±è´¥ : é›†ç¾¤ä¸å­˜åœ¨å¯ç”¨èŠ‚ç‚¹ï¼Œè¯·åˆ‡æ¢ä¸€ä¸ªé›†ç¾¤"));
            }

            // éšæœºé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹
            ClusterNodeEntity agentNode = clusterNodes.get(new Random().nextInt(clusterNodes.size()));
            logBuilder.append(infoLog("ğŸ‘Œ ç”³è¯·èµ„æºå®Œæˆï¼Œæ¿€æ´»èŠ‚ç‚¹: " + agentNode.getName()));

            // è§£æè¯·æ±‚èŠ‚ç‚¹ä¿¡æ¯
            ScpFileEngineNodeDto scpNode = clusterNodeMapper.engineNodeEntityToScpFileEngineNodeDto(agentNode);
            scpNode.setPasswd(aesUtils.decrypt(scpNode.getPasswd()));

            // ä¿å­˜äº‹ä»¶
            workRunContext.setClusterType(cluster.getClusterType());
            workRunContext.setScpNodeInfo(scpNode);
            workRunContext.setAgentNode(agentNode);

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹æ£€æµ‹è„šæœ¬"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // è§£æSqlè„šæœ¬
        if (workEvent.getEventProcess() == 2) {

            // æ£€æŸ¥è„šæœ¬æ˜¯å¦ä¸ºç©º
            if (Strings.isEmpty(workRunContext.getScript())) {
                throw new WorkRunException(errorLog("âš ï¸ æ£€æµ‹è„šæœ¬å¤±è´¥ : SQLå†…å®¹ä¸ºç©ºä¸èƒ½æ‰§è¡Œ"));
            }

            // å»æ‰sqlä¸­çš„æ³¨é‡Š
            String sqlNoComment = sqlCommentService.removeSqlComment(workRunContext.getScript());

            // è§£æä¸Šæ¸¸ç»“æœ
            String jsonPathSql = parseJsonPath(sqlNoComment, workInstance);

            // è§£æç³»ç»Ÿå˜é‡
            String parseValueSql = sqlValueService.parseSqlValue(jsonPathSql);

            // è§£æç³»ç»Ÿå‡½æ•°
            String script = sqlFunctionService.parseSqlFunction(parseValueSql);

            // è§£æå…¨å±€å˜é‡
            List<SecretKeyEntity> allKey = secretKeyRepository.findAll();
            for (SecretKeyEntity secretKeyEntity : allKey) {
                script = script.replace("${{ secret." + secretKeyEntity.getKeyName() + " }}",
                    secretKeyEntity.getSecretValue());
            }

            // ä¿å­˜äº‹ä»¶
            workRunContext.setScript(script);

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("ğŸ‘Œ è§£æSparkSqlå®Œæˆ:"));
            logBuilder.append(script).append("\n");
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹ä¸Šä¼ è‡ªå®šä¹‰å‡½æ•°"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // ä¸Šä¼ è‡ªå®šä¹‰å‡½æ•°
        if (workEvent.getEventProcess() == 3) {

            // ä¸Šä¼ è‡ªå®šä¹‰å‡½æ•°
            if (workRunContext.getFuncConfig() != null) {

                // ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
                ScpFileEngineNodeDto scpNode = workRunContext.getScpNodeInfo();
                ClusterNodeEntity agentNode = workRunContext.getAgentNode();

                // å‡½æ•°æ–‡ä»¶ç›®å½•
                String funcDir = PathUtils.parseProjectPath(isxAppProperties.getResourcesPath()) + File.separator
                    + "file" + File.separator + workInstance.getTenantId();
                List<FuncEntity> allFunc = funcRepository.findAllById(workRunContext.getFuncConfig());
                allFunc.forEach(e -> {
                    try {
                        scpJar(scpNode, funcDir + File.separator + e.getFileId(),
                            agentNode.getAgentHomePath() + "/zhiqingyun-agent/file/" + e.getFileId() + ".jar");
                    } catch (JSchException | SftpException | InterruptedException | IOException ex) {
                        throw new WorkRunException(errorLog("âš ï¸  è‡ªå®šä¹‰å‡½æ•°jaræ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆ–è€…é‡æ–°ä¸Šä¼ "));
                    }
                });
            }

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("ğŸ‘Œ ä¸Šä¼ è‡ªå®šä¹‰å‡½æ•°å®Œæˆ"));
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹ä¸Šä¼ è‡ªå®šä¹‰ä¾èµ–"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // ä¸Šä¼ è‡ªå®šä¹‰ä¾èµ–
        if (workEvent.getEventProcess() == 4) {

            // ä¸Šä¼ è‡ªå®šä¹‰ä¾èµ–
            if (workRunContext.getLibConfig() != null) {

                // ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
                ScpFileEngineNodeDto scpNode = workRunContext.getScpNodeInfo();
                ClusterNodeEntity agentNode = workRunContext.getAgentNode();

                // ä¾èµ–æ–‡ä»¶ç›®å½•
                String libDir = PathUtils.parseProjectPath(isxAppProperties.getResourcesPath()) + File.separator
                    + "file" + File.separator + workInstance.getTenantId();
                List<FileEntity> libFile = fileRepository.findAllById(workRunContext.getLibConfig());
                libFile.forEach(e -> {
                    try {
                        scpJar(scpNode, libDir + File.separator + e.getId(),
                            agentNode.getAgentHomePath() + "/zhiqingyun-agent/file/" + e.getId() + ".jar");
                    } catch (JSchException | SftpException | InterruptedException | IOException ex) {
                        throw new WorkRunException(errorLog("âš ï¸  è‡ªå®šä¹‰ä¾èµ–jaræ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆ–è€…é‡æ–°ä¸Šä¼ "));
                    }
                });
            }

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("ğŸ‘Œ ä¸Šä¼ è‡ªå®šä¹‰ä¾èµ–å®Œæˆ"));
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹æ„å»ºä½œä¸šè¯·æ±‚ä½“"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // æ„å»ºä½œä¸šè¯·æ±‚ä½“
        if (workEvent.getEventProcess() == 5) {

            // ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
            String script = workRunContext.getScript();
            String clusterType = workRunContext.getClusterType();
            ClusterNodeEntity agentNode = workRunContext.getAgentNode();

            // æ„å»ºè¯·æ±‚ä½“
            SubmitWorkReq submitWorkReq = new SubmitWorkReq();
            submitWorkReq.setWorkId(workRunContext.getWorkId());
            submitWorkReq.setWorkType(WorkType.QUERY_SPARK_SQL);
            submitWorkReq.setWorkInstanceId(workInstance.getId());
            submitWorkReq.setAgentHomePath(agentNode.getAgentHomePath() + "/" + PathConstants.AGENT_PATH_NAME);
            submitWorkReq.setSparkHomePath(agentNode.getSparkHomePath());
            submitWorkReq.setClusterType(clusterType);

            // æ„é€ sparkæäº¤è¯·æ±‚ä½“
            SparkSubmit sparkSubmit = SparkSubmit.builder().verbose(true)
                .mainClass("com.isxcode.spark.plugin.query.sql.Execute").appResource("spark-query-sql-plugin.jar")
                .conf(genSparkSubmitConfig(workRunContext.getClusterConfig().getSparkConfig())).build();

            // æ„å»ºæ’ä»¶è¯·æ±‚ä½“
            PluginReq pluginReq = PluginReq.builder().sql(script).limit(200)
                .sparkConfig(genSparkConfig(workRunContext.getClusterConfig().getSparkConfig())).build();

            // è‡ªå®šä¹‰å‡½æ•°
            if (workRunContext.getFuncConfig() != null) {
                List<FuncEntity> allFunc = funcRepository.findAllById(workRunContext.getFuncConfig());
                pluginReq.setFuncInfoList(funcMapper.funcEntityListToFuncInfoList(allFunc));
                submitWorkReq.setFuncConfig(funcMapper.funcEntityListToFuncInfoList(allFunc));
            }

            // è‡ªå®šä¹‰ä¾èµ–
            if (workRunContext.getLibConfig() != null) {
                submitWorkReq.setLibConfig(workRunContext.getLibConfig());
            }

            // é…ç½®hiveæ•°æ®æº
            if (StringUtils.isNotBlank(workRunContext.getDatasourceId())
                && workRunContext.getClusterConfig().getEnableHive()) {

                DatasourceEntity datasourceEntity = datasourceService.getDatasource(workRunContext.getDatasourceId());
                ConnectInfo connectInfo = datasourceMapper.datasourceEntityToConnectInfo(datasourceEntity);
                Datasource datasource = dataSourceFactory.getDatasource(connectInfo.getDbType());
                pluginReq.setDatabase(datasource.parseDbName(datasourceEntity.getJdbcUrl()));
                pluginReq.getSparkConfig().put("hive.metastore.uris", datasourceEntity.getMetastoreUris());

                // æ·»åŠ è‡ªå®šä¹‰username
                if (Strings.isNotBlank(datasourceEntity.getUsername())) {
                    sparkSubmit.getConf().put("qing.hive.username", datasourceEntity.getUsername());
                }
            }

            // ä¿å­˜è¯·æ±‚ä½“
            submitWorkReq.setSparkSubmit(sparkSubmit);
            submitWorkReq.setPluginReq(pluginReq);
            workRunContext.setSubmitWorkReq(submitWorkReq);

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("ğŸ‘Œ æ„å»ºä½œä¸šå®Œæˆ"));
            workRunContext.getClusterConfig().getSparkConfig()
                .forEach((k, v) -> logBuilder.append(k).append(":").append(v).append(" \n"));
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹æäº¤ä½œä¸š"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // æäº¤ä½œä¸š
        if (workEvent.getEventProcess() == 6) {

            // ä¸Šä¸‹æ–‡ä¸­è·å–å‚æ•°
            SubmitWorkReq submitWorkReq = workRunContext.getSubmitWorkReq();
            ClusterNodeEntity agentNode = workRunContext.getAgentNode();

            // å¼€å§‹æäº¤ä½œä¸š
            try {
                BaseResponse<?> baseResponse = HttpUtils.doPost(httpUrlUtils.genHttpUrl(agentNode.getHost(),
                    agentNode.getAgentPort(), SparkAgentUrl.SUBMIT_WORK_URL), submitWorkReq, BaseResponse.class);
                if (!String.valueOf(HttpStatus.OK.value()).equals(baseResponse.getCode())
                    || baseResponse.getData() == null) {
                    throw new WorkRunException(errorLog("âš ï¸ æäº¤ä½œä¸šå¤±è´¥ : " + baseResponse.getMsg()));
                }
                RunWorkRes submitWorkRes =
                    JSON.parseObject(JSON.toJSONString(baseResponse.getData()), RunWorkRes.class);
                logBuilder.append(infoLog("ğŸ‘Œ æäº¤ä½œä¸šæˆåŠŸ : " + submitWorkRes.getAppId()));

                // ä¿å­˜å®ä¾‹
                workInstance.setSparkStarRes(JSON.toJSONString(submitWorkRes));

                // ä¿å­˜ä¸Šä¸‹æ–‡
                workRunContext.setAppId(submitWorkRes.getAppId());
            } catch (ResourceAccessException e) {
                log.error(e.getMessage(), e);
                throw new WorkRunException(errorLog("âš ï¸ æäº¤ä½œä¸šå¤±è´¥ : " + e.getMessage()));
            } catch (HttpServerErrorException e1) {
                log.error(e1.getMessage(), e1);
                if (HttpStatus.BAD_GATEWAY.value() == e1.getRawStatusCode()) {
                    throw new WorkRunException(errorLog("âš ï¸ æäº¤ä½œä¸šå¤±è´¥ : æ— æ³•è®¿é—®èŠ‚ç‚¹æœåŠ¡å™¨,è¯·æ£€æŸ¥æœåŠ¡å™¨é˜²ç«å¢™æˆ–è€…è®¡ç®—é›†ç¾¤"));
                }
                throw new WorkRunException(errorLog("âš ï¸ æäº¤ä½œä¸šå¤±è´¥ : " + e1.getMessage()));
            }

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹ç›‘å¬çŠ¶æ€"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // ç›‘å¬ä½œä¸šçŠ¶æ€
        if (workEvent.getEventProcess() == 7) {

            // æäº¤ä½œä¸šæˆåŠŸåï¼Œå¼€å§‹å¾ªç¯åˆ¤æ–­çŠ¶æ€
            String appId = workRunContext.getAppId();
            String clusterType = workRunContext.getClusterType();
            ClusterNodeEntity agentNode = workRunContext.getAgentNode();
            String preStatus = workRunContext.getPreStatus() == null ? "" : workRunContext.getPreStatus();

            // è·å–ä½œä¸šçŠ¶æ€å¹¶ä¿å­˜
            GetWorkStatusReq getWorkStatusReq = GetWorkStatusReq.builder().appId(appId).clusterType(clusterType)
                .sparkHomePath(agentNode.getSparkHomePath()).build();
            BaseResponse<?> baseResponse = HttpUtils.doPost(httpUrlUtils.genHttpUrl(agentNode.getHost(),
                agentNode.getAgentPort(), SparkAgentUrl.GET_WORK_STATUS_URL), getWorkStatusReq, BaseResponse.class);
            if (!String.valueOf(HttpStatus.OK.value()).equals(baseResponse.getCode())) {
                throw new WorkRunException(errorLog("âš ï¸ è·å–ä½œä¸šçŠ¶æ€å¼‚å¸¸ : " + baseResponse.getMsg()));
            }

            // è§£æè¿”å›çŠ¶æ€
            RunWorkRes workStatusRes = JSON.parseObject(JSON.toJSONString(baseResponse.getData()), RunWorkRes.class);

            // å¦‚æœä½œä¸šçŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œåˆ™ä¿å­˜çŠ¶æ€
            if (!preStatus.equals(workStatusRes.getAppStatus())) {
                logBuilder.append(infoLog("â© è¿è¡ŒçŠ¶æ€: " + workStatusRes.getAppStatus()));

                // æ›´æ–°å®ä¾‹
                workInstance.setSparkStarRes(JSON.toJSONString(workStatusRes));
                updateInstance(workInstance, logBuilder);

                // æ›´æ–°ä¸Šä¸‹æ–‡
                workRunContext.setPreStatus(workStatusRes.getAppStatus());
                updateWorkEvent(workEvent, workRunContext);
            }

            // å¦‚æœæ˜¯è¿è¡Œä¸­çŠ¶æ€ï¼Œç›´æ¥è¿”å›
            List<String> runningStatus =
                Arrays.asList("RUNNING", "UNDEFINED", "SUBMITTED", "CONTAINERCREATING", "PENDING");
            if (runningStatus.contains(workStatusRes.getAppStatus().toUpperCase())) {
                return InstanceStatus.RUNNING;
            }

            // å¦‚æœæ˜¯ä¸­æ­¢ï¼Œç›´æ¥é€€å‡º
            List<String> abortStatus = Arrays.asList("KILLED", "TERMINATING");
            if (abortStatus.contains(workStatusRes.getAppStatus().toUpperCase())) {
                throw new WorkRunException(errorLog("âš ï¸ ä½œä¸šè¿è¡Œä¸­æ­¢"));
            }

            // å…¶ä»–çŠ¶æ€åˆ™ä¸ºè¿è¡Œç»“æŸ
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹ä¿å­˜ä½œä¸šæ—¥å¿—å’Œæ•°æ®"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // ä¿å­˜ä½œä¸šæ—¥å¿—å’Œæ•°æ®
        if (workEvent.getEventProcess() == 8) {

            // è·å–ä¸Šä¸‹æ–‡
            String appId = workRunContext.getAppId();
            String clusterType = workRunContext.getClusterType();
            String preStatus = workRunContext.getPreStatus();
            ClusterNodeEntity agentNode = workRunContext.getAgentNode();

            // è·å–æ—¥å¿—å¹¶ä¿å­˜
            GetWorkStderrLogReq getWorkStderrLogReq = GetWorkStderrLogReq.builder().appId(appId)
                .clusterType(clusterType).sparkHomePath(agentNode.getSparkHomePath()).build();
            BaseResponse<?> baseResponse =
                HttpUtils.doPost(httpUrlUtils.genHttpUrl(agentNode.getHost(), agentNode.getAgentPort(),
                    SparkAgentUrl.GET_WORK_STDERR_LOG_URL), getWorkStderrLogReq, BaseResponse.class);

            if (!String.valueOf(HttpStatus.OK.value()).equals(baseResponse.getCode())) {
                throw new WorkRunException(errorLog("âš ï¸ è·å–ä½œä¸šæ—¥å¿—å¼‚å¸¸ : " + baseResponse.getMsg()));
            }

            // è§£ææ—¥å¿—å¹¶ä¿å­˜
            GetWorkStderrLogRes yagGetLogRes =
                JSON.parseObject(JSON.toJSONString(baseResponse.getData()), GetWorkStderrLogRes.class);
            workInstance.setYarnLog(yagGetLogRes.getLog());
            logBuilder.append(infoLog("ğŸ‘Œ æ—¥å¿—ä¿å­˜æˆåŠŸ"));

            // å¦‚æœè¿è¡ŒæˆåŠŸï¼Œè¿˜è¦ç»§ç»­ä¿å­˜æ•°æ®
            List<String> successStatus = Arrays.asList("FINISHED", "SUCCEEDED", "COMPLETED");
            if (successStatus.contains(preStatus.toUpperCase())) {

                // è·å–æ•°æ®
                GetWorkDataReq getWorkDataReq = GetWorkDataReq.builder().appId(appId).clusterType(clusterType)
                    .sparkHomePath(agentNode.getSparkHomePath()).build();
                baseResponse = HttpUtils.doPost(httpUrlUtils.genHttpUrl(agentNode.getHost(), agentNode.getAgentPort(),
                    SparkAgentUrl.GET_WORK_DATA_URL), getWorkDataReq, BaseResponse.class);
                if (!String.valueOf(HttpStatus.OK.value()).equals(baseResponse.getCode())) {
                    throw new WorkRunException(errorLog("âš ï¸ è·å–ä½œä¸šæ•°æ®å¼‚å¸¸ : " + baseResponse.getErr()));
                }

                // è§£ææ•°æ®å¹¶ä¿å­˜
                workInstance.setResultData(JSON.toJSONString(baseResponse.getData()));

                // ä¿å­˜æ—¥å¿—
                logBuilder.append(infoLog("ğŸ‘Œ æ•°æ®ä¿å­˜æˆåŠŸ"));
            } else {

                // å…¶ä»–çŠ¶æ€ä¸ºå¤±è´¥
                workRunContext.setPreStatus(InstanceStatus.FAIL);
            }

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("âŒ›ï¸ å¼€å§‹æ¸…ç†æ‰§è¡Œæ–‡ä»¶"));
            return updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // æ¸…ç†ä½œä¸šæ‰§è¡Œæ–‡ä»¶
        if (workEvent.getEventProcess() == 9) {

            // è·å–ä¸Šä¸‹æ–‡
            String appId = workRunContext.getAppId();
            String clusterType = workRunContext.getClusterType();
            ClusterNodeEntity agentNode = workRunContext.getAgentNode();

            // k8sä½œä¸šè¦å…³é—­ä½œä¸š
            if (AgentType.K8S.equals(clusterType)) {
                StopWorkReq stopWorkReq = StopWorkReq.builder().appId(appId).clusterType(AgentType.K8S)
                    .sparkHomePath(agentNode.getAgentHomePath()).agentHomePath(agentNode.getAgentHomePath()).build();
                HttpUtils.doPost(
                    httpUrlUtils.genHttpUrl(agentNode.getHost(), agentNode.getPort(), SparkAgentUrl.STOP_WORK_URL),
                    stopWorkReq, BaseResponse.class);

            }

            // ä¿å­˜æ—¥å¿—
            logBuilder.append(infoLog("ğŸ‘Œ æ¸…ç†æ‰§è¡Œæ–‡ä»¶å®Œæˆ"));
            updateWorkEventAndInstance(workInstance, logBuilder, workEvent, workRunContext);
        }

        // å¦‚æœæœ€ç»ˆçŠ¶æ€ä¸ºå¤±è´¥ï¼ŒæŠ›å‡ºç©ºå¼‚å¸¸
        if (InstanceStatus.FAIL.equals(workRunContext.getPreStatus())) {
            throw new WorkRunException(LocalDateTime.now() + WorkLog.ERROR_INFO + "âš ï¸ ä½œä¸šæœ€ç»ˆçŠ¶æ€ä¸ºå¤±è´¥");
        }

        return InstanceStatus.SUCCESS;
    }

    @Override
    protected void abort(WorkInstanceEntity workInstance) {

        // åˆ¤æ–­ä½œä¸šæœ‰æ²¡æœ‰æäº¤æˆåŠŸ
        locker.lock("REQUEST_" + workInstance.getId());
        try {
            workInstance = workInstanceRepository.findById(workInstance.getId()).get();
            if (!Strings.isEmpty(workInstance.getSparkStarRes())) {
                RunWorkRes wokRunWorkRes = JSON.parseObject(workInstance.getSparkStarRes(), RunWorkRes.class);
                if (!Strings.isEmpty(wokRunWorkRes.getAppId())) {
                    // å…³é—­è¿œç¨‹çº¿ç¨‹
                    WorkEntity work = workRepository.findById(workInstance.getWorkId()).get();
                    WorkConfigEntity workConfig = workConfigRepository.findById(work.getConfigId()).get();
                    ClusterConfig clusterConfig = JSON.parseObject(workConfig.getClusterConfig(), ClusterConfig.class);
                    List<ClusterNodeEntity> allEngineNodes = clusterNodeRepository
                        .findAllByClusterIdAndStatus(clusterConfig.getClusterId(), ClusterNodeStatus.RUNNING);
                    if (allEngineNodes.isEmpty()) {
                        throw new WorkRunException(
                            LocalDateTime.now() + WorkLog.ERROR_INFO + "ç”³è¯·èµ„æºå¤±è´¥ : é›†ç¾¤ä¸å­˜åœ¨å¯ç”¨èŠ‚ç‚¹ï¼Œè¯·åˆ‡æ¢ä¸€ä¸ªé›†ç¾¤  \n");
                    }
                    ClusterEntity cluster = clusterRepository.findById(clusterConfig.getClusterId()).get();

                    // èŠ‚ç‚¹é€‰æ‹©éšæœºæ•°
                    ClusterNodeEntity engineNode = allEngineNodes.get(new Random().nextInt(allEngineNodes.size()));
                    StopWorkReq stopWorkReq = StopWorkReq.builder().appId(wokRunWorkRes.getAppId())
                        .clusterType(cluster.getClusterType()).sparkHomePath(engineNode.getSparkHomePath())
                        .agentHomePath(engineNode.getAgentHomePath()).build();
                    BaseResponse<?> baseResponse = HttpUtils.doPost(httpUrlUtils.genHttpUrl(engineNode.getHost(),
                        engineNode.getAgentPort(), SparkAgentUrl.STOP_WORK_URL), stopWorkReq, BaseResponse.class);

                    if (!String.valueOf(HttpStatus.OK.value()).equals(baseResponse.getCode())) {
                        throw new IsxAppException(baseResponse.getCode(), baseResponse.getMsg(), baseResponse.getErr());
                    }
                } else {
                    // å…ˆæ€æ­»è¿›ç¨‹
                    WORK_THREAD.get(workInstance.getId()).interrupt();
                }
            }
        } finally {
            locker.clearLock("REQUEST_" + workInstance.getId());
        }
    }

    public Map<String, String> genSparkSubmitConfig(Map<String, String> sparkConfig) {

        // è¿‡æ»¤æ‰ï¼Œå‰ç¼€ä¸åŒ…å«spark.xxxçš„é…ç½®ï¼Œspark submitä¸­å¿…é¡»éƒ½æ˜¯spark.xxx
        Map<String, String> sparkSubmitConfig = new HashMap<>();
        sparkConfig.forEach((k, v) -> {
            if (k.startsWith("spark")) {
                sparkSubmitConfig.put(k, v);
            }
        });
        return sparkSubmitConfig;
    }

    public Map<String, String> genSparkConfig(Map<String, String> sparkConfig) {

        // k8sçš„é…ç½®ä¸èƒ½æäº¤åˆ°ä½œä¸šä¸­
        sparkConfig.remove("spark.kubernetes.driver.podTemplateFile");
        sparkConfig.remove("spark.kubernetes.executor.podTemplateFile");

        return sparkConfig;
    }
}

