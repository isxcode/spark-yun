#### 前提

- [hadoop 安装](http://ispong.isxcode.com/hadoop/hadoop/hadoop%20%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85/)
- [开启hadoop jobhistory服务](http://ispong.isxcode.com/hadoop/hadoop/hadoop%20Jobhistory/)
- [spark-yun 安装](./linux源码安装.md)

#### 启动容器

> 通过docker启动容器

```bash
docker run --name spark-yun -p 30175:8080 -d isxcode/zhiqingyun:0.0.1
```

- 访问地址：http://localhost:30175
- 默认账号：ispong
- 默认密码：ispong123

#### 创建计算引擎

> 计算引擎，每个作业运行都是基于计算引擎运行

![20230404105752](https://img.isxcode.com/picgo/20230404105752.png)

#### 添加节点

> 计算引擎支持集群模式，可以添加多个计算节点

![20230404105834](https://img.isxcode.com/picgo/20230404105834.png)

![20230404105940](https://img.isxcode.com/picgo/20230404105940.png)

#### 检测节点可安装

> 检测节点是否可以安装至轻云代理

![20230404110129](https://img.isxcode.com/picgo/20230404110129.png)

#### 节点安装

> 节点检测通过，可以安装至轻云代理

![20230404110209](https://img.isxcode.com/picgo/20230404110209.png)

#### 创建作业流

> 创建作业流

![20230404110249](https://img.isxcode.com/picgo/20230404110249.png)

#### 添加sparkSql作业

> 创建作业

![20230404110311](https://img.isxcode.com/picgo/20230404110311.png)

![20230404110351](https://img.isxcode.com/picgo/20230404110351.png)

#### 配置作业

> 配置作业的计算引擎

![20230404110458](https://img.isxcode.com/picgo/20230404110458.png)

![20230404110527](https://img.isxcode.com/picgo/20230404110527.png)

#### 编写sql

> 编写自己的sparksql

![20230404110734](https://img.isxcode.com/picgo/20230404110734.png)

#### 作业运行

> 提交日志返回提交成功

![20230404113729](https://img.isxcode.com/picgo/20230404113729.png)

#### 查看运行状态

![20230404114944](https://img.isxcode.com/picgo/20230404114944.png)

#### 查看数据返回

![20230404115007](https://img.isxcode.com/picgo/20230404115007.png)

#### 查看日志

![20230404115046](https://img.isxcode.com/picgo/20230404115046.png)

#### 查看作业web ui

![20230404115126](https://img.isxcode.com/picgo/20230404115126.png)
